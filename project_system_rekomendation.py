# -*- coding: utf-8 -*-
"""project_system_rekomendation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ERoqks93HEHXmMex8MjOrBtNesDKyCFQ

# Pendahuluan

Proyek ini bertujuan untuk membangun sistem rekomendasi film menggunakan dua pendekatan utama: Collaborative Filtering dan Content-Based Filtering. Dataset yang digunakan berasal dari Kaggle netflix movie rating dataset, yang terdiri dari data rating pengguna dan movie detail.

Sistem ini bertujuan untuk:

- Memberikan rekomendasi film yang relevan bagi pengguna berdasarkan riwayat rating mereka (Collaborative Filtering).

- Memberikan rekomendasi film berdasarkan kemiripan konten (judul) film (Content-Based Filtering).

# import
"""

!pip install numpy==1.24.4 scikit-surprise --force-reinstall

"""**Insight:** menginstall numpy dan scikit-surprise agar tidak conflict"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from surprise import Dataset, Reader, SVD
from surprise.model_selection import train_test_split
from surprise import accuracy
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel
import re
import string

"""**Insight:** Melakukan instalasi paket scikit-surprise untuk algoritma SVD dan import berbagai library untuk analisis data dan modelling."""

from google.colab import files
files.upload()  # upload kaggle.json kamu

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# Download kaggle dataset and unzip the file
# !cp kaggle.json ~/.kaggle/

# !chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets download -d rishitjavia/netflix-movie-rating-dataset
!unzip netflix-movie-rating-dataset.zip

"""**Insight:**
Mengatur autentikasi dan mengunduh dataset Netflix dari Kaggle yaitu dari akun @rishitjavia. File yang didapat terdiri dari Netflix_Dataset_Movie.csv dan Netflix_Dataset_Rating.csv.

# Data Loading

### Data Rating
"""

rating_df = pd.read_csv('/content/Netflix_Dataset_Rating.csv', header=0)
rating_df

"""**Insight:**
- Membaca file CSV berisi data rating ke dalam DataFrame.
-   Dataset rating memiliki 17337458 baris dan 3 kolom.
- User_ID adalah nomer penonton
- Rating adalah penilaian film dari 1-5
- Movie_ID adalah nomer film
- tidak memiliki missing value
- tidak memiliki duplikat

### Data Movie
"""

movie_df = pd.read_csv('/content/Netflix_Dataset_Movie.csv', header=0)
movie_df

"""**Insight:**
- Membaca file CSV berisi data movie ke dalam DataFrame.
-   Dataset memiliki 17770 baris (pengamatan) dan 3 kolom.
- Name adalah nama film
- Year adalah tahun film
- Movie_ID adalah nomer film
- tidak memiliki missing value
- tidak memiliki duplikat

# Data  Understading

### Data Rating
"""

rating_df.info()

"""
**Insight:** rating_df memiliki kolom User_ID, Rating, Movie_ID  dan ketganya memiliki type data int64

"""

rating_df.describe()

"""
**Insight:** rating_df memiliki kolom:
- Count: 17,3 juta rating dari ~2,65 juta pengguna untuk ~4.496 film.
- Mean: Rata-rata User_ID ~1,32 juta, rating 3,59 (skala 1-5), Movie_ID ~2.303.
- Std: Variasi User_ID 764.692, rating 1,06, Movie_ID 1.303, menunjukkan distribusi yang cukup merata.
- Min-Max: User_ID (6 - 2,65 juta), rating (1 - 5), Movie_ID (3 - 4.496).

"""

rating_df.nunique()

"""Fitur unik:
- User_ID:143458 data
- Rating: Skor penillaian 1-5.
- Movie_ID: 1350 data.
"""

rating_df['Rating'].value_counts()

"""### Memeriksa Null value"""

rating_df.isnull().sum()

"""**Insight:** tidak memiliki data yag kosong pada rating_df

### Memeriksa duplikasi
"""

rating_df.duplicated().sum()

"""### Data Movie"""

movie_df.info()

"""
**Insight:** movie_df memiliki kolom Movie_ID memiliki tipe data int64, Year memiliki tipe data int64, Name memiliki tipe data object
"""

movie_df.describe()

"""Insight:
- Count: 17.770 film.
- Mean: Movie_ID rata-rata 8.885,5; tahun rilis rata-rata 1990,24.
- Std: Variasi Movie_ID 5.129,9; tahun 16,56, menunjukkan sebaran luas.
- Min-Max: Movie_ID (1 - 17.770); tahun (1915 - 2005).
"""

movie_df.nunique()

"""Fitur:
- Movie_ID: 17770 data ID film.
- Year: 91 jumlah Tahun rilis.
- Name: 17297 data Judul film.
"""

movie_df['Name'].value_counts()

"""**Insight:** Jumlah data terbayak dari setiap data film

### Memeriksa Null value
"""

movie_df.isnull().sum()

"""**Insight:**ternyata tidak memiliki data kosong pada movie_df"""

movie_df.duplicated().sum()

"""### Distribusi Data"""

# Distribusi rating
plt.figure(figsize=(8,5))
sns.histplot(rating_df['Rating'], bins=5)
plt.title('Distribusi Rating')
plt.xlabel('Rating')
plt.ylabel('Jumlah')
plt.show()

"""**Insight:** Jumlah distribusi data menunjukkan banyak data rating bintang 4"""

# Gabungkan file rating dan movie
rating_movie = rating_df.merge(movie_df, on='Movie_ID')

top_movies = movie_df['Name'].value_counts().head(10)

plt.figure(figsize=(12,6))
sns.barplot(x=top_movies.index, y=top_movies.values)
plt.title('Top 10 Film dengan jumlah terbanyak')
plt.xlabel('Film')
plt.xticks(rotation=60, ha='right')
plt.tight_layout()
plt.show()

"""**Insight:** Jumlah distribusi data film jumlah muncul terbanyak.

### Memeriksa duplikasi
"""

print(rating_df.duplicated().sum())

"""**Insight:**tidak ada data duplicate pada rating_df"""

print(movie_df.duplicated().sum())

"""**Insight:**tidak ada data duplicate pada movie_df

### Cek Outlier
"""

def iqr_outlier(df):
  outlier_indices = {}
  for col in df.select_dtypes(include=['int64', 'float64']).columns:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    outliers = rating_df[(df[col] < lower_bound) | (df[col] > upper_bound)]
    outlier_indices[col] = outliers
  return outlier_indices

outlier_dict = iqr_outlier(rating_df)
# Menampilkan jumlah outlier per kolom
for col, outliers in outlier_dict.items():
    print(f"Outliers pada kolom '{col}': {len(outliers)} baris")

def iqr_outlier(df):
  outlier_indices = {}
  for col in df.select_dtypes(include=['int64', 'float64']).columns:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    outliers = movie_df[(df[col] < lower_bound) | (df[col] > upper_bound)]
    outlier_indices[col] = outliers
  return outlier_indices

outlier_dict = iqr_outlier(movie_df)
# Menampilkan jumlah outlier per kolom
for col, outliers in outlier_dict.items():
    print(f"Outliers pada kolom '{col}': {len(outliers)} baris")

"""**Insight:** Metode IQR mendeteksi nilai rendah seperti 1 sebagai outlier secara statistik, bukan karena nilai itu salah, tapi karena distribusi data yang tidak seimbang (kebanyakan nilai 3–5).


"""

for col in ['User_ID', 'Rating', 'Movie_ID']:
    sns.boxplot(x=rating_df[col])
    plt.title(f'Boxplot for {col}')
    plt.savefig(f'boxplot_{col}.png')
    plt.show()

"""**Insight:** Semua kolom terlihat bersih, tidak ada outlier signifikan secara statistik, untuk rating hanya sedikit outlier yang menjadi titik pada boxplotnya namun nilai rendah seperti 1 sebagai outlier secara statistik, bukan karena nilai itu salah, tapi karena distribusi data yang tidak seimbang (kebanyakan nilai 3–5)


"""

for col in ['Movie_ID', 'Year']:
    sns.boxplot(x=movie_df[col])
    plt.title(f'Boxplot for {col}')
    plt.savefig(f'boxplot_{col}.png')
    plt.show()

"""**insight:** tidak digunakan sebagai fitur, outlier tidak akan berdampak signifikan.

# Data Preparation

### Handling Missing Value

**Insight:** Karena tidak ada yang null pada kedua dataset, maka tidak diperlukan proses imputasi atau penghapusan data akibat missing value. Hal ini mempercepat proses data preparation dan memastikan bahwa seluruh data dapat digunakan secara maksimal dalam proses training dan evaluasi model.

### Handling Duplicates

**Insight:** Karena tidak ada duplicate maka tidak diperlukan proses penghapusan data ganda. Ini menunjukkan bahwa data yang tersedia telah bersih dari redundansi dan siap digunakan untuk proses analisis dan pemodelan tanpa risiko perhitungan ganda yang dapat memengaruhi hasil.

### Handling Outliers

**Insight:** Karena outlier pada rating bukan karena nilai itu salah, tapi karena distribusi data yang tidak seimbang (kebanyakan nilai 3–5), maka tidak diperlukan proses tindakan handling outlier karena data itu juga penting dalam model nantinya. dan pada movie_df dataset movie terdapat outliers pada year namun tidak digunakan sebagai fitur, outlier tidak akan berdampak signifikan, jika dibuang berpotensi kehilangan film klasik atau penting secara historis.

### Merge data
"""

df = pd.merge(rating_df, movie_df, on='Movie_ID')
df.head(10)

"""**Insight:**Menggbungkan data rating_df dan movie_df dan menampilkan 10 data

## Collaborative Filltering Preparation
"""

# Gunakan hanya kolom yang dibutuhkan
data_df = df[['User_ID', 'Movie_ID', 'Rating']]

# Reader butuh tahu rentang rating
reader = Reader(rating_scale=(1, 5))

# Format data ke dalam objek Dataset surprise
data = Dataset.load_from_df(data_df, reader)

"""**Insight:** Data difokuskan hanya pada kolom yang relevan untuk model Collaborative Filtering, lalu dikonversi ke format yang sesuai dengan library surprise untuk keperluan training dan evaluasi.

### Split Data
"""

from surprise.model_selection import train_test_split

trainset, testset = train_test_split(data, test_size=0.2, random_state=42)

"""**Insight:** Data dibagi menjadi train dan test set untuk mengevaluasi kinerja model secara objektif, dengan random_state untuk hasil yang konsisten..

## Collaborative Filltering Preparation
"""

# Pakai hanya film unik
movie_features = movie_df[['Movie_ID', 'Name', 'Year']].drop_duplicates()
# Hilangkan nilai kosong (jika ada)
movie_features['Name'] = movie_features['Name'].fillna('')
movie_features.sample(5)

"""**Insight:** mengunakan nilai unik saja dan mengganti nilai Nan dengan nilai kosong"""

# Fungsi preprocessing teks
def preprocess_text(text):
    text = text.lower()  # Lowercase
    text = re.sub(r'\d+', '', text)  # Hapus angka
    text = text.translate(str.maketrans('', '', string.punctuation))  # Hapus tanda baca
    text = text.strip()  # Hapus spasi di awal/akhir
    return text

movie_features = movie_features.reset_index(drop=True)

# Terapkan preprocessing pada kolom judul film
movie_features['Clean_Name'] = movie_features['Name'].apply(preprocess_text)

# Inisialisasi TF-IDF Vectorizer
tfidf = TfidfVectorizer(stop_words='english')

# Fit dan transform teks yang sudah dibersihkan
tfidf_matrix = tfidf.fit_transform(movie_features['Clean_Name'])

"""**Insight:** Judul film diubah ke bentuk numerik menggunakan TF-IDF, yang memungkinkan model mengenali kemiripan konten antar film berdasarkan teks judulnya."""

# Index dataframe berdasarkan Movie_ID
indices = pd.Series(movie_features.index, index=movie_features['Movie_ID']).drop_duplicates()
movie_id_to_index = dict(zip(movie_features['Movie_ID'], movie_features.index))

"""**Insight:** Membuat Series yang memetakan Movie_ID ke indeks baris DataFrame movie_features, menghapus duplicate, dan Membuat kamus (dictionary) yang memetakan Movie_ID ke indeks baris DataFrame.

# Model and Results

## Collaborative Filtering - SVD
"""

model = SVD(n_epochs=10,verbose = True)
model.fit(trainset)

"""**Insight**
Melatih model SVD dengan epoch=10 dan verbose true
"""

# Langkah 1: Ambil semua Movie_ID yang belum di-rating oleh user 712664
user_id = 2643029

# Movie yang sudah dirating oleh user ini
rated_movies = data_df[data_df['User_ID'] == user_id]['Movie_ID'].tolist()

# Movie yang belum dirating
all_movies = data_df['Movie_ID'].unique()
unrated_movies = [movie for movie in all_movies if movie not in rated_movies]

# Langkah 2: Prediksi rating untuk semua movie yang belum dirating
user_predictions = [model.predict(user_id, movie_id) for movie_id in unrated_movies]

# Langkah 3: Ambil top-N (misal 10) dengan rating tertinggi
top_n_preds = sorted(user_predictions, key=lambda x: x.est, reverse=True)[:10]

# Langkah 4: Ambil informasi judul dan tahun dari movie_df
top_n_result = []
for pred in top_n_preds:
    movie_info = movie_df[movie_df['Movie_ID'] == int(pred.iid)].iloc[0]
    top_n_result.append({
        'Year': movie_info['Year'],
        'Name': movie_info['Name'],
        'Estimated_Rating': round(pred.est, 4)
    })

# Langkah 5: Tampilkan hasil rekomendasi
top_n_df = pd.DataFrame(top_n_result)
print(top_n_df)

user_id = 43441
rated_movies = data_df[data_df['User_ID'] == user_id]['Movie_ID'].tolist()
all_movies = data_df['Movie_ID'].unique()
unrated_movies = [movie for movie in all_movies if movie not in rated_movies]
user_predictions = [model.predict(user_id, movie_id) for movie_id in unrated_movies]
top_n_preds = sorted(user_predictions, key=lambda x: x.est, reverse=True)[:10]

top_n_result = []
for pred in top_n_preds:
    movie_info = movie_df[movie_df['Movie_ID'] == int(pred.iid)].iloc[0]
    top_n_result.append({
        'Year': movie_info['Year'],
        'Name': movie_info['Name'],
        'Estimated_Rating': round(pred.est, 4)
    })

top_n_df = pd.DataFrame(top_n_result)
print(top_n_df)

"""**Insight**: Memprediksi rating untuk film yang belum ditonton oleh user, dan menampilkan rekomendasi 10 film terbaik.

## Content-based Filtering
"""

# Hitung similarity antar film berdasarkan judul
cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)

"""**Insight:** Mengubah judul film menjadi representasi numerik menggunakan TF-IDF dan menghitung kesamaan antar film."""

# Fungsi untuk memberikan rekomendasi berdasarkan movie_id
def get_recommendations(movie_id, cosine_sim, movie_id_to_index, movie_df, top_n=10):
    # Cek apakah movie_id ada dalam mapping
    if movie_id not in movie_id_to_index:
        return f"Movie_ID {movie_id} tidak ditemukan dalam data."

    idx = movie_id_to_index[movie_id]

    # Ambil skor similarity untuk film tersebut
    sim_scores = list(enumerate(cosine_sim[idx]))

    # Urutkan berdasarkan skor similarity dan ambil top_n
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:top_n+1]

    # Ambil indeks film yang relevan berdasarkan skor similarity
    movie_indices = [i[0] for i in sim_scores]
    similarity_scores = [i[1] for i in sim_scores]

    # Ambil informasi film yang relevan berdasarkan indeks
    recommended_movies = movie_df.iloc[movie_indices][['Movie_ID', 'Name', 'Year']]

    # Tambahkan kolom Similarity ke dalam hasil rekomendasi
    recommended_movies['Similarity'] = similarity_scores

    # Urutkan berdasarkan Similarity
    recommended_movies = recommended_movies.sort_values(by='Similarity', ascending=False)

    return recommended_movies[['Movie_ID', 'Name', 'Year', 'Similarity']]

"""**Insight:** Fungsi untuk menghasilkan 10 film yang paling mirip dengan film tertentu berdasarkan kemiripan judul."""

# Contoh pemanggilan fungsi rekomendasi berdasarkan movie_id
recommendations = get_recommendations(3456, cosine_sim, movie_id_to_index, movie_features)
print(recommendations)

"""**Insight:** menggunakan fungsi rekomendasi dengan movie_id= dan menampilkan 10 film"""

movie_df.loc[movie_df['Movie_ID'] == 3456, ['Name', 'Year']]

"""**Insight:** menampilkan nama movie_id dengan nomer 3456

# Evaluasi

### Collaborative Filtering - SVD
"""

predictions = model.test(testset)
rmse = accuracy.rmse(predictions)
mae = accuracy.mae(predictions)

print("RMSE:", rmse)
print("MAE:", mae)

"""**Insight:** Menghitung metrik RMSE dan MAE untuk mengevaluasi akurasi prediksi model Collaborative Filtering.

### Content-based Filtering
"""

!pip install fuzzywuzzy

"""**Insight:**
Menggunakan fuzzywuzzy untuk menghitung kesamaan nama antar film secara parsial. Ini digunakan sebagai ground truth untuk mengevaluasi seberapa akurat rekomendasi Content-Based berdasarkan judul mirip.


"""

from fuzzywuzzy import fuzz

reference_movie_id = 3456  # Movie_ID untuk "Lost: Season 1"
if reference_movie_id in movie_df['Movie_ID'].values:
    reference_idx = movie_id_to_index[reference_movie_id]
    reference_name = movie_df.loc[reference_idx, 'Name']
    print(f"Reference Movie ID: {reference_movie_id}, Name: {reference_name}")

    # Ground truth: film dengan skor kesamaan nama > 80
    # Gunakan apply untuk menghitung skor kesamaan untuk setiap judul
    similarity_scores = movie_df['Name'].apply(lambda x: fuzz.partial_ratio(x, reference_name))
    relevant_movies = set(movie_df[similarity_scores > 80]['Movie_ID'])
    print(f"Jumlah film relevan (nama mirip): {len(relevant_movies)}")

    # Dapatkan rekomendasi
    recommended_df = get_recommendations(3456, cosine_sim, movie_id_to_index, movie_features)
    print("Rekomendasi:", recommended_df)
    recommended = set(recommended_df['Movie_ID'])

    # Precision@k
    def precision_at_k(recommended, relevant, k):
        recommended_k = set(list(recommended)[:k])
        return len(recommended_k & relevant) / k if recommended_k else 0

    k = 10
    precision = precision_at_k(recommended, relevant_movies, k)
    print(f"Precision@{k}: {precision:.4f}")

    # Recall@k
    def recall_at_k(recommended, relevant, k):
        recommended_k = set(list(recommended)[:k])
        return len(recommended_k & relevant) / len(relevant) if relevant else 0

    k = 10
    recall = recall_at_k(recommended, relevant_movies, k)
    print(f"Recall@{k}: {recall:.4f}")

    # Hit Rate
    def hit_rate(recommended, relevant):
        return 1 if recommended & relevant else 0

    hit = hit_rate(recommended, relevant_movies)
    print(f"Hit Rate: {hit:.4f}")
else:
    print(f"Movie_ID {reference_movie_id} tidak ditemukan!")

"""**Insight:**
- Menentukan film yang dianggap relevan berdasarkan kemiripan judul ≥ 80 menggunakan metode partial_ratio dari fuzzy matching.
- Precision@k mengukur proporsi film yang direkomendasikan dalam top-k dan benar-benar relevan menurut fuzzy matching
- Recall@k mengukur proporsi film relevan yang berhasil ditangkap oleh sistem rekomendasi dalam top-k rekomendasi.
-Hit Rate bernilai 1 jika setidaknya satu film dalam rekomendasi termasuk dalam ground truth (film relevan).

# Kesimpulan

### Hasil Evaluasi
Evaluasi model **Collaborative Filtering (SVD)** menghasilkan **nilai RMSE** sebesar **0.8684** dan **MAE** sebesar **0.6782**, yang menunjukkan bahwa **model ini cukup baik dalam memprediksi rating pengguna terhadap film**. Sementara itu, **Content-Based Filtering** yang dievaluasi menggunakan **Precision@10** sebesar **0.3**, **Recall@10** sebesar **0.0323**, dan **Hit Rate** sebesar **1.0**, menunjukkan bahwa sistem mampu merekomendasikan film yang cukup relevan berdasarkan kemiripan judul, meskipun cakupannya terhadap semua film relevan masih terbatas.
"""